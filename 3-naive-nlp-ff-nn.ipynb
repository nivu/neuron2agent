{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Problems\n",
    "1. Variable Sequence Length\n",
    "2. Order Matters\n",
    "3. Short and Long Term Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'am', 'and', 'at', 'bad', 'earlier', 'good', 'happy', 'i', 'is', 'not', 'now', 'or', 'right', 'sad', 'this', 'very', 'was']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "# finding number of unique words\n",
    "\n",
    "unique_words = set()\n",
    "\n",
    "for key in train_data.keys():\n",
    "    unique_words.update(key.split())\n",
    "\n",
    "unique_words = list(sorted(unique_words))\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] good\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] bad\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] happy\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] sad\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] not good\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] not bad\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] not happy\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0] not sad\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] very good\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] very bad\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0] very happy\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0] very sad\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] i am happy\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0] this is good\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] i am bad\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0] this is bad\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0] i am sad\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0] this is sad\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] i am not happy\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0] this is not good\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] i am not bad\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0] this is not sad\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0] i am very happy\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0] this is very good\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0] i am very bad\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0] this is very sad\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0] this is very happy\n",
      "[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] i am good not bad\n",
      "[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0] this is good not bad\n",
      "[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] i am bad not good\n",
      "[0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] i am good and happy\n",
      "[0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0] this is not good and not happy\n",
      "[1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] i am not at all good\n",
      "[1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] i am not at all bad\n",
      "[1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0] i am not at all happy\n",
      "[1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0] this is not at all sad\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0] this is not at all happy\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0] i am good right now\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0] i am bad right now\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0] this is bad right now\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0] i am sad right now\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1] i was good earlier\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1] i was happy earlier\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1] i was bad earlier\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1] i was sad earlier\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0] i am very bad right now\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0] this is very good right now\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0] this is very sad right now\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1] this was bad earlier\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1] this was very good earlier\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1] this was very bad earlier\n",
      "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1] this was very happy earlier\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1] this was very sad earlier\n",
      "[0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1] i was good and not bad earlier\n",
      "[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1] i was not good and not happy earlier\n",
      "[1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0] i am not at all bad or sad right now\n",
      "[1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0] i am not at all good or happy right now\n",
      "[0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1] this was not happy and not good earlier\n"
     ]
    }
   ],
   "source": [
    "# sentence to bag of words\n",
    "X_train = []\n",
    "for key in train_data.keys():\n",
    "    vector = [0] * len(unique_words)\n",
    "    for word in key.split():\n",
    "        vector[unique_words.index(word)] = vector[unique_words.index(word)] + 1\n",
    "    X_train.append(vector)\n",
    "\n",
    "for v, k in zip(X_train, train_data.keys()):\n",
    "    print(v, k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "Y_train = list(map(int, train_data.values()))\n",
    "\n",
    "print(Y_train)\n",
    "print(len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4916],\n",
      "        [0.5295],\n",
      "        [0.5084],\n",
      "        [0.5057],\n",
      "        [0.4930],\n",
      "        [0.5139],\n",
      "        [0.4987],\n",
      "        [0.4860],\n",
      "        [0.4917],\n",
      "        [0.5190],\n",
      "        [0.5131],\n",
      "        [0.4968],\n",
      "        [0.5196],\n",
      "        [0.5062],\n",
      "        [0.5373],\n",
      "        [0.5419],\n",
      "        [0.5138],\n",
      "        [0.5082],\n",
      "        [0.5100],\n",
      "        [0.5056],\n",
      "        [0.5324],\n",
      "        [0.4940],\n",
      "        [0.5216],\n",
      "        [0.5090],\n",
      "        [0.5380],\n",
      "        [0.5038],\n",
      "        [0.5282],\n",
      "        [0.5163],\n",
      "        [0.5183],\n",
      "        [0.5163],\n",
      "        [0.4723],\n",
      "        [0.4884],\n",
      "        [0.5160],\n",
      "        [0.5416],\n",
      "        [0.5335],\n",
      "        [0.5029],\n",
      "        [0.5301],\n",
      "        [0.4842],\n",
      "        [0.5135],\n",
      "        [0.5301],\n",
      "        [0.4836],\n",
      "        [0.5300],\n",
      "        [0.5267],\n",
      "        [0.5365],\n",
      "        [0.5321],\n",
      "        [0.5154],\n",
      "        [0.5144],\n",
      "        [0.5007],\n",
      "        [0.5720],\n",
      "        [0.5343],\n",
      "        [0.5603],\n",
      "        [0.5333],\n",
      "        [0.5292],\n",
      "        [0.4991],\n",
      "        [0.4815],\n",
      "        [0.5071],\n",
      "        [0.5081],\n",
      "        [0.4810]], device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "class OneNeuronSequential(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(len(unique_words), 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = OneNeuronSequential().to(device)\n",
    "print(model(X_train))\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.02934490516781807\n",
      "Epoch 100 loss: 0.023916341364383698\n",
      "Epoch 200 loss: 0.023910056799650192\n",
      "Epoch 300 loss: 0.02390734665095806\n",
      "Epoch 400 loss: 0.023905755952000618\n",
      "Epoch 500 loss: 0.023904701694846153\n",
      "Epoch 600 loss: 0.023904018104076385\n",
      "Epoch 700 loss: 0.023903602734208107\n",
      "Epoch 800 loss: 0.02390327863395214\n",
      "Epoch 900 loss: 0.023903027176856995\n",
      "Epoch 1000 loss: 0.023902826011180878\n",
      "Epoch 1100 loss: 0.023902656510472298\n",
      "Epoch 1200 loss: 0.023902527987957\n",
      "Epoch 1300 loss: 0.023902658373117447\n",
      "Epoch 1400 loss: 0.023902345448732376\n",
      "Epoch 1500 loss: 0.023902352899312973\n",
      "Epoch 1600 loss: 0.02390223741531372\n",
      "Epoch 1700 loss: 0.023902133107185364\n",
      "Epoch 1800 loss: 0.02390209212899208\n",
      "Epoch 1900 loss: 0.023902375251054764\n",
      "Epoch 2000 loss: 0.023903999477624893\n",
      "Epoch 2100 loss: 0.023902546614408493\n",
      "Epoch 2200 loss: 0.02390226162970066\n",
      "Epoch 2300 loss: 0.02390209771692753\n",
      "Epoch 2400 loss: 0.023901987820863724\n",
      "Epoch 2500 loss: 0.023901917040348053\n",
      "Epoch 2600 loss: 0.023901866748929024\n",
      "Epoch 2700 loss: 0.02390182577073574\n",
      "Epoch 2800 loss: 0.023901795968413353\n",
      "Epoch 2900 loss: 0.023902807384729385\n",
      "Epoch 3000 loss: 0.02390175312757492\n",
      "Epoch 3100 loss: 0.023916153237223625\n",
      "Epoch 3200 loss: 0.02390306442975998\n",
      "Epoch 3300 loss: 0.023902256041765213\n",
      "Epoch 3400 loss: 0.023902012035250664\n",
      "Epoch 3500 loss: 0.02390189655125141\n",
      "Epoch 3600 loss: 0.02390182577073574\n",
      "Epoch 3700 loss: 0.023901784792542458\n",
      "Epoch 3800 loss: 0.023906530812382698\n",
      "Epoch 3900 loss: 0.02390173263847828\n",
      "Epoch 4000 loss: 0.023903001099824905\n",
      "Epoch 4100 loss: 0.023901715874671936\n",
      "Epoch 4200 loss: 0.023901695385575294\n",
      "Epoch 4300 loss: 0.023902732878923416\n",
      "Epoch 4400 loss: 0.0239016804844141\n",
      "Epoch 4500 loss: 0.023902324959635735\n",
      "Epoch 4600 loss: 0.023901674896478653\n",
      "Epoch 4700 loss: 0.023901665583252907\n",
      "Epoch 4800 loss: 0.023902079090476036\n",
      "Epoch 4900 loss: 0.02390165999531746\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def evaluate(model, loss_fn, X, Y):\n",
    "    with torch.no_grad():\n",
    "        prediction = model(X)\n",
    "        loss = loss_fn(prediction, Y.reshape(-1, 1))\n",
    "        return loss\n",
    "\n",
    "# training loop\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(X_train)\n",
    "    loss = loss_fn(prediction, Y_train.reshape(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} loss: {evaluate(model, loss_fn, X_train, Y_train)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00],\n",
      "        [6.1014e-09],\n",
      "        [1.0000e+00],\n",
      "        [2.7288e-09],\n",
      "        [9.7908e-10],\n",
      "        [1.0000e+00],\n",
      "        [3.5088e-10],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [6.5161e-12],\n",
      "        [1.0000e+00],\n",
      "        [6.0000e-13],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.8906e-09],\n",
      "        [1.3886e-08],\n",
      "        [4.2506e-10],\n",
      "        [3.4529e-09],\n",
      "        [1.9628e-10],\n",
      "        [9.7441e-08],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [4.2992e-12],\n",
      "        [4.0380e-13],\n",
      "        [1.0000e+00],\n",
      "        [5.0109e-01],\n",
      "        [1.0000e+00],\n",
      "        [5.0109e-01],\n",
      "        [1.0000e+00],\n",
      "        [3.0585e-14],\n",
      "        [4.0352e-13],\n",
      "        [1.0000e+00],\n",
      "        [3.1140e-14],\n",
      "        [1.0000e+00],\n",
      "        [3.1137e-10],\n",
      "        [1.0000e+00],\n",
      "        [9.8709e-13],\n",
      "        [5.8748e-13],\n",
      "        [9.8153e-14],\n",
      "        [1.0000e+00],\n",
      "        [1.0000e+00],\n",
      "        [1.3385e-10],\n",
      "        [6.2125e-12],\n",
      "        [5.2078e-13],\n",
      "        [1.0000e+00],\n",
      "        [1.4669e-15],\n",
      "        [9.5293e-12],\n",
      "        [1.0000e+00],\n",
      "        [9.6710e-14],\n",
      "        [1.0000e+00],\n",
      "        [7.7886e-15],\n",
      "        [1.0000e+00],\n",
      "        [2.9166e-13],\n",
      "        [1.0000e+00],\n",
      "        [3.4739e-17],\n",
      "        [1.0107e-13]], device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(model(X_train))\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "tensor([1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for op in model(X_train):\n",
    "    if op > 0.5:\n",
    "        results.append(1)\n",
    "    else:\n",
    "        results.append(0)\n",
    "\n",
    "print(results)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/navaneethmalingan/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from scikit-learn) (2.2.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.1-cp312-cp312-macosx_14_0_arm64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "tensor([1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827586206896551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(results, Y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31  0]\n",
      " [ 1 26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(results, Y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        31\n",
      "           1       1.00      0.96      0.98        27\n",
      "\n",
      "    accuracy                           0.98        58\n",
      "   macro avg       0.98      0.98      0.98        58\n",
      "weighted avg       0.98      0.98      0.98        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(results, Y_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'am', 'and', 'at', 'bad', 'earlier', 'good', 'happy', 'i', 'is', 'not', 'now', 'or', 'right', 'sad', 'this', 'very', 'was']\n"
     ]
    }
   ],
   "source": [
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "new_ip_good = \"i am good not bad at all\"\n",
    "new_ip_bad = \"i am bad not good at all\"\n",
    "\n",
    "new_ip_good_vector = [0] * len(unique_words)\n",
    "for word in new_ip_good.split():\n",
    "    new_ip_good_vector[unique_words.index(word)] = new_ip_good_vector[unique_words.index(word)] + 1\n",
    "\n",
    "new_ip_bad_vector = [0] * len(unique_words)\n",
    "for word in new_ip_bad.split():\n",
    "    new_ip_bad_vector[unique_words.index(word)] = new_ip_bad_vector[unique_words.index(word)] + 1\n",
    "\n",
    "print(new_ip_good_vector)\n",
    "print(new_ip_bad_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9800], device='mps:0', grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.9800], device='mps:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor(new_ip_good_vector, dtype=torch.float32).to(device)))\n",
    "print(model(torch.tensor(new_ip_bad_vector, dtype=torch.float32).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
