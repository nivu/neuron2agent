# neuron2agent


| **Stage** | **Year** | **Model/Architecture**      | **Key Techniques/Concepts**                                             | **Training Methodology**                                              | **New Layers/Functions/Methods Introduced**                          |
| --------- | -------- | --------------------------- | ----------------------------------------------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------- |
| 1         | 1943     | Neural Network              | Basic feed-forward neural networks.                                     | Supervised learning with gradient descent.                            | Neurons, feed-forward layers, backpropagation.                       |
| 2         | 1986     | Autoencoders                | Feature compression and reconstruction.                                 | Unsupervised training to minimize reconstruction loss.                | Encoder and decoder layers, latent space representation.             |
| 3         | 1990     | Elman RNN                   | Simple recurrent neural network with feedback loops.                    | Supervised sequence learning.                                         | Recurrent connections, feedback loops.                               |
| 4         | 1997     | Bi-directional RNN          | Processes sequences in both forward and backward directions.            | Supervised learning with bidirectional context.                       | Bi-directional recurrent layers.                                     |
| 5         | 1997     | LSTM                        | Overcame vanishing gradient problems with gating mechanisms.            | Supervised learning with gating mechanisms for sequence data.         | Forget gate, input gate, output gate, cell state.                    |
| 6         | 2014     | Gated Recurrent Unit (GRU)  | Simplified LSTM with fewer parameters.                                  | Supervised sequence learning with gating mechanisms.                  | Update gate, reset gate.                                             |
| 7         | 2014     | Seq2Seq                     | Encoder-decoder structure for sequence generation.                      | Supervised learning with teacher forcing for translation tasks.       | Encoder-decoder layers.                                              |
| 8         | 2015     | Seq2Seq with Attention      | Improved Seq2Seq with attention mechanisms for better context handling. | Supervised learning with attention weights during training.           | Attention mechanism, alignment scores.                               |
| 9         | 2017     | Self-Attention Transformers | Attention-only mechanism replacing recurrence.                          | Pretraining with masked language models or autoregressive loss.       | Multi-head self-attention, positional encoding, layer normalization. |
| 10        | 2018     | [[BERT]]                    | Bidirectional transformer with masked language modeling.                | Pretraining with masked language modeling and fine-tuning.            | Transformer encoder, masked language modeling objective.             |
| 11        | 2018     | [[GPT]]                     | Autoregressive transformer for language generation.                     | Pretraining with causal language modeling and fine-tuning.            | Transformer decoder, causal attention mask.                          |
| 12        | 2019     | [[transformer]] XL          | Handles long-term dependencies with segment-level recurrence.           | Pretraining with autoregressive objectives and memory segments.       | Recurrence-based memory mechanism, relative positional encoding.     |
| 13        | 2019     | XLNet                       | Permutation-based autoregressive pretraining.                           | Pretraining with permutation-based objectives.                        | Permutation-based training, two-stream self-attention.               |
| 14        | 2019     | RoBERTa                     | Optimized BERT with larger datasets and training strategies.            | Pretraining with masked language modeling on larger corpora.          | Dynamic masking, larger mini-batches, optimized hyperparameters.     |
| 15        | 2019     | ALBERT                      | Light BERT with parameter sharing to reduce memory usage.               | Pretraining with masked language modeling and knowledge distillation. | Factorized embeddings, cross-layer parameter sharing.                |
| 16        | 2020     | GPT-2                       | Larger-scale GPT with better generation capabilities.                   | Pretraining with causal language modeling and scaling laws.           | Scaled transformer decoder layers.                                   |
| 17        | 2020     | T5                          | Unified text-to-text framework for various NLP tasks.                   | Pretraining with span-based denoising and supervised fine-tuning.     | Text-to-text objective, span-based masking.                          |
| 18        | 2020     | BART                        | Combines bidirectional (BERT) and autoregressive (GPT) approaches.      | Pretraining with denoising autoencoding tasks.                        | Bidirectional encoder and autoregressive decoder combination.        |
| 19        | 2020     | GPT-3                       | Large-scale GPT with massive parameters for better generalization.      | Pretraining with causal language modeling on large datasets.          | Scaling laws for model size and training data.                       |
| 20        | 2022     | PaLM                        | Pathways framework for training efficiency and multimodal capabilities. | Pretraining with large-scale autoregressive modeling.                 | Sparsely activated networks, multitask learning.                     |
| 21        | 2022     | Chinchilla                  | Optimized compute vs. data trade-off for efficiency.                    | Pretraining with smaller models and more tokens.                      | Trade-off between compute and data scale, token optimization.        |
| 22        | 2022     | OPT                         | Open-source large-scale autoregressive transformer.                     | Pretraining with causal language modeling.                            | Model optimization for energy efficiency.                            |
| 23        | 2023     | [[LLAMA]]                   | Optimized, efficient large language model for scaling and fine-tuning.  | Pretraining on diverse datasets with autoregressive objectives.       | Low-rank adaptation (LoRA), efficient fine-tuning methods.           |
